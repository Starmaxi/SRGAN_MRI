{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code für Bild im .png Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import seaborn as sns\n",
    "import lpips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'srgan_generator.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Pfad zum Modell\u001b[39;00m\n\u001b[32m     13\u001b[39m generator = Generator()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m generator.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msrgan_generator.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     15\u001b[39m generator.eval()\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 2. LR-Bild vorbereiten\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\torch\\serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\torch\\serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\torch\\serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'srgan_generator.pth'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Generator laden\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        #...\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        #...\n",
    "        pass\n",
    "\n",
    "# Pfad zum Modell\n",
    "generator = Generator()\n",
    "generator.load_state_dict(torch.load(\"srgan_generator.pth\", map_location=\"cpu\"))\n",
    "generator.eval()\n",
    "\n",
    "# 2. LR-Bild vorbereiten\n",
    "\n",
    "# Transform: skalieren & Tensor erzeugen (für LPIPS wichtig: normalize NICHT nötig)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # lpips ist NN das dementsprechend immer gleiche Bildgröße braucht\n",
    "    transforms.Grayscale(num_output_channels=3),  #3 statt 1\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Bilder laden\n",
    "lr_img = Image.open(\"example_lr.png\").convert(\"L\")  # L = Greyscale\n",
    "hr_img = Image.open(\"example_hr.png\").convert(\"L\")\n",
    "\n",
    "# In 3-Kanal-Tensoren umwandeln\n",
    "lr_tensor = transform(lr_img).unsqueeze(0)  # Shape: (1, 3, H, W)\n",
    "hr_tensor = transform(hr_img).unsqueeze(0)\n",
    "\n",
    "# 3. Inferenz; generator auf lr anwenden und super resolution erzeugen\n",
    "with torch.no_grad():\n",
    "    sr_tensor = generator(lr_tensor)\n",
    "\n",
    "# 4. numpy arrays erstellen\n",
    "sr_np = sr_tensor.squeeze().permute(1, 2, 0).numpy()\n",
    "hr_np = hr_tensor.squeeze().permute(1, 2, 0).numpy()\n",
    "\n",
    "# Konvertieren auf [0,1] für metriken\n",
    "sr_np = np.clip(sr_np, 0, 1)\n",
    "hr_np = np.clip(hr_np, 0, 1)\n",
    "\n",
    "psnr_value = psnr(hr_np, sr_np, data_range=1.0)\n",
    "ssim_value = ssim(hr_np, sr_np, multichannel=True, data_range=1.0)\n",
    "\n",
    "print(f\"PSNR: {psnr_value:.2f} dB\")\n",
    "print(f\"SSIM: {ssim_value:.4f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(lr_img)\n",
    "plt.title(\"Low Resolution\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(np.clip(sr_np, 0, 1))\n",
    "plt.title(\"Super-Resolved\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(hr_img)\n",
    "plt.title(\"High Resolution\")\n",
    "\n",
    "plt.suptitle(f\"PSNR: {psnr_value:.2f} dB, SSIM: {ssim_value:.4f}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hr_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#MAE + MSE \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m mae_value = np.mean(np.abs(\u001b[43mhr_np\u001b[49m - sr_np))\n\u001b[32m      4\u001b[39m mse_value = np.mean((hr_np - sr_np) ** \u001b[32m2\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'hr_np' is not defined"
     ]
    }
   ],
   "source": [
    "#MAE + MSE \n",
    "\n",
    "mae_value = np.mean(np.abs(hr_np - sr_np))\n",
    "mse_value = np.mean((hr_np - sr_np) ** 2)\n",
    "\n",
    "print(f\"MAE: {mae_value:.4f}\")\n",
    "print(f\"MSE: {mse_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hr_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#heatmap bzw errormap (differenz der heatmaps)\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Nur Luminanz/Intensitätskanal, falls gewünscht\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m hr_gray = np.mean(\u001b[43mhr_np\u001b[49m, axis=\u001b[32m2\u001b[39m)\n\u001b[32m      5\u001b[39m sr_gray = np.mean(sr_np, axis=\u001b[32m2\u001b[39m)\n\u001b[32m      6\u001b[39m error_map = np.abs(hr_gray - sr_gray)\n",
      "\u001b[31mNameError\u001b[39m: name 'hr_np' is not defined"
     ]
    }
   ],
   "source": [
    "#heatmap bzw errormap (differenz der heatmaps)\n",
    "\n",
    "# Nur Luminanz/Intensitätskanal, falls gewünscht\n",
    "hr_gray = np.mean(hr_np, axis=2)\n",
    "sr_gray = np.mean(sr_np, axis=2)\n",
    "error_map = np.abs(hr_gray - sr_gray)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(error_map, cmap=\"viridis\", cbar=True)\n",
    "plt.title(\"Error Heatmap (|HR - SR|)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hr_gray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#grey value histogram\u001b[39;00m\n\u001b[32m      3\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m plt.hist(\u001b[43mhr_gray\u001b[49m.ravel(), bins=\u001b[32m50\u001b[39m, alpha=\u001b[32m0.5\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mHR\u001b[39m\u001b[33m\"\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33mred\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m plt.hist(sr_gray.ravel(), bins=\u001b[32m50\u001b[39m, alpha=\u001b[32m0.5\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mSR\u001b[39m\u001b[33m\"\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mHistogramm der Grauwertverteilung\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'hr_gray' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#grey value histogram\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(hr_gray.ravel(), bins=50, alpha=0.5, label=\"HR\", color='red')\n",
    "plt.hist(sr_gray.ravel(), bins=50, alpha=0.5, label=\"SR\", color='blue')\n",
    "plt.title(\"Histogramm der Grauwertverteilung\")\n",
    "plt.xlabel(\"Intensität\")\n",
    "plt.ylabel(\"Pixelanzahl\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\LaraR/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "3.4%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#LPIPS: Bewertet Unterschiede im feature Raum\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Modell laden (z. B. VGG basierend)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m lpips_fn = \u001b[43mlpips\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLPIPS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvgg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Optionen: 'alex', 'vgg', 'squeeze'\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Eingabebilder müssen (1, 3, H, W) und normalisiert [-1, 1] sein\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_lpips_tensor\u001b[39m(img_np):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\lpips\\lpips.py:84\u001b[39m, in \u001b[36mLPIPS.__init__\u001b[39m\u001b[34m(self, pretrained, net, version, lpips, spatial, pnet_rand, pnet_tune, use_dropout, model_path, eval_mode, verbose)\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mself\u001b[39m.chns = [\u001b[32m64\u001b[39m,\u001b[32m128\u001b[39m,\u001b[32m256\u001b[39m,\u001b[32m384\u001b[39m,\u001b[32m384\u001b[39m,\u001b[32m512\u001b[39m,\u001b[32m512\u001b[39m]\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.L = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.chns)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m.net = \u001b[43mnet_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpnet_rand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpnet_tune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(lpips):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m.lin0 = NetLinLayer(\u001b[38;5;28mself\u001b[39m.chns[\u001b[32m0\u001b[39m], use_dropout=use_dropout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\lpips\\pretrained_networks.py:99\u001b[39m, in \u001b[36mvgg16.__init__\u001b[39m\u001b[34m(self, requires_grad, pretrained)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, requires_grad=\u001b[38;5;28;01mFalse\u001b[39;00m, pretrained=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     98\u001b[39m     \u001b[38;5;28msuper\u001b[39m(vgg16, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     vgg_pretrained_features = \u001b[43mtv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvgg16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m)\u001b[49m.features\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m.slice1 = torch.nn.Sequential()\n\u001b[32m    101\u001b[39m     \u001b[38;5;28mself\u001b[39m.slice2 = torch.nn.Sequential()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[39m, in \u001b[36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m     warnings.warn(\n\u001b[32m    136\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs.keys()),\u001b[38;5;250m \u001b[39mseparate_last=\u001b[33m'\u001b[39m\u001b[33mand \u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as positional \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minstead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m     kwargs.update(keyword_only_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[39m, in \u001b[36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[32m    226\u001b[39m     kwargs[weights_param] = default_weights_arg\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\torchvision\\models\\vgg.py:433\u001b[39m, in \u001b[36mvgg16\u001b[39m\u001b[34m(weights, progress, **kwargs)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"VGG-16 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__.\u001b[39;00m\n\u001b[32m    414\u001b[39m \n\u001b[32m    415\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m \u001b[33;03m    :members:\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    431\u001b[39m weights = VGG16_Weights.verify(weights)\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vgg\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\torchvision\\models\\vgg.py:105\u001b[39m, in \u001b[36m_vgg\u001b[39m\u001b[34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     model.load_state_dict(\u001b[43mweights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\torchvision\\models\\_api.py:90\u001b[39m, in \u001b[36mWeightsEnum.get_state_dict\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\torch\\hub.py:867\u001b[39m, in \u001b[36mload_state_dict_from_url\u001b[39m\u001b[34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[39m\n\u001b[32m    865\u001b[39m         r = HASH_REGEX.search(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[32m    866\u001b[39m         hash_prefix = r.group(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LaraR\\projekt\\Lib\\site-packages\\torch\\hub.py:744\u001b[39m, in \u001b[36mdownload_url_to_file\u001b[39m\u001b[34m(url, dst, hash_prefix, progress)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\n\u001b[32m    737\u001b[39m     total=file_size,\n\u001b[32m    738\u001b[39m     disable=\u001b[38;5;129;01mnot\u001b[39;00m progress,\n\u001b[32m   (...)\u001b[39m\u001b[32m    741\u001b[39m     unit_divisor=\u001b[32m1024\u001b[39m,\n\u001b[32m    742\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m         buffer = \u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREAD_DATA_CHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    745\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) == \u001b[32m0\u001b[39m:\n\u001b[32m    746\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:465\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    463\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    464\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m s = \u001b[38;5;28mself\u001b[39m.fp.read(amt)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:705\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    707\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1275\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1276\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1277\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1134\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#LPIPS: Bewertet Unterschiede im feature Raum\n",
    "\n",
    "# Modell laden (z. B. VGG basierend)\n",
    "lpips_fn = lpips.LPIPS(net='vgg')  # Optionen: 'alex', 'vgg', 'squeeze'\n",
    "\n",
    "# Eingabebilder müssen (1, 3, H, W) und normalisiert [-1, 1] sein\n",
    "def to_lpips_tensor(img_np):\n",
    "    img_tensor = torch.tensor(img_np).permute(2, 0, 1).unsqueeze(0)\n",
    "    img_tensor = img_tensor * 2 - 1  # [0,1] → [-1,1]\n",
    "    return img_tensor.float()\n",
    "\n",
    "sr_lpips = to_lpips_tensor(sr_np)\n",
    "hr_lpips = to_lpips_tensor(hr_np)\n",
    "\n",
    "# LPIPS-Distanz berechnen\n",
    "lpips_distance = lpips_fn(sr_lpips, hr_lpips).item()\n",
    "print(f\"LPIPS Distance: {lpips_distance:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
